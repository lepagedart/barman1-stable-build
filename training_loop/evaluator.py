import os
import openai
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

INPUT_DIR = "training_loop/scenario_outputs"
OUTPUT_DIR = "training_loop/evaluator_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

EVALUATOR_SYSTEM_PROMPT = """You are an experienced AI bar consultant trainer. Your task is to critically review responses generated by another assistant (named Lloyd) to structured bar-program scenarios. You must simulate an internal critique‚Äînot for the end user‚Äîfocused on improving Lloyd‚Äôs performance over time.

Each input will include the scenario prompt, the assistant‚Äôs full reply, and relevant metadata such as scenario type and operational context.

Your output should clearly fill in the ‚ÄúWhat Lloyd Should Have Done‚Äù section with direct, constructive feedback. You may reference framework principles (e.g. batching logic, cocktail codex, menu balance, KPIs), but avoid excessive verbosity.  
- Flag if Lloyd confuses staff constraints with cocktail creativity.  
- If he defaults to ‚Äúbasic/generic‚Äù cocktails when more creative but equally efficient builds are possible, call this out as a failure of imagination.  
- Require him to reference prepped/KB house cocktails where relevant instead of generic templates.
- Prioritize clarity, precision, and actionable suggestions. Assume this critique will be ingested back into Lloyd‚Äôs knowledge base and used to train future responses.
"""

def extract_fields_from_txt(content):
    """
    Extracts metadata, prompt, and Lloyd's response from a .txt file.
    Assumes field headings like: Title:, Tags:, Prompt:, Lloyd's Response:, etc.
    """
    fields = {}
    current_key = None
    lines = content.splitlines()

    for line in lines:
        if line.endswith(":") and not line.startswith(" "):
            current_key = line[:-1].strip()
            fields[current_key] = ""
        elif current_key:
            fields[current_key] += line + "\n"

    return fields


def evaluate_with_openai(scenario_fields):
    """
    Sends Lloyd‚Äôs output and scenario context to GPT-4o for internal critique.
    """
    content = f"""Scenario Type: {scenario_fields.get("Scenario Type", "").strip()}
Tags: {scenario_fields.get("Tags", "").strip()}
System Mod: {scenario_fields.get("System Mod", "").strip()}
Venue Context: {scenario_fields.get("Venue Context", "").strip()}

Prompt: {scenario_fields.get("Prompt", "").strip()}

Lloyd's Response:
{scenario_fields.get("Lloyd's Response", "").strip()}
"""

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": EVALUATOR_SYSTEM_PROMPT},
            {"role": "user", "content": content}
        ],
        temperature=0.4
    )

    content = response.choices[0].message.content
    return content.strip() if content is not None else ""


def main():
    for filename in os.listdir(INPUT_DIR):
        if not filename.endswith(".json"):
            continue

        filepath = os.path.join(INPUT_DIR, filename)
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        if "What Lloyd Should Have Done:" in content and "[PLACEHOLDER]" not in content:
            print(f"‚úÖ Already evaluated: {filename}")
            continue

        fields = extract_fields_from_txt(content)
        critique = evaluate_with_openai(fields)

        new_content = content.replace("[PLACEHOLDER]", critique)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_content += f"\n\n(Evaluated on {timestamp})"

        output_path = os.path.join(OUTPUT_DIR, filename)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(new_content)

        print(f"üí¨ Evaluated and saved: {filename}")

if __name__ == "__main__":
    main()